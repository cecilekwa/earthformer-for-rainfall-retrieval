{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10402907-b451-4342-86c5-9fe5f93a0bf4",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97de44e-1160-482b-9239-c7a37943f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eumdac\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import requests\n",
    "import time\n",
    "import fnmatch\n",
    "from satpy import Scene\n",
    "from pyresample import create_area_def\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Proj\n",
    "from pyresample import kd_tree\n",
    "#from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "import h5py\n",
    "import os\n",
    "import re\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "\n",
    "from pathos.threading import ThreadPool as Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f4c16-feea-476c-81cf-67d9d9934ef3",
   "metadata": {},
   "source": [
    "# Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ededa2f9-7b18-4fba-ba79-96911182355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUTDIR of the hdf5 files\n",
    "OUTPUTDIR = 'set_your_desired_output_dir_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35568f7-6023-49cf-b876-0f7ddab46633",
   "metadata": {},
   "source": [
    "# Set API credentials\n",
    "\n",
    "### Tokens can be found here: https://api.eumetsat.int/api-key/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19def16-2505-4d1f-8cac-2c90aff8afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This token 'a9553c79-9ef4-372e-9ca5-f680851f5ec9' expires 2025-03-06 10:29:09.957907\n"
     ]
    }
   ],
   "source": [
    "# Insert your personal key and secret into the single quotes\n",
    "consumer_key = 'your_consumer_key' #own google email adres account\n",
    "consumer_secret = 'your_consumer_secret' #own google email adres account\n",
    "\n",
    "credentials = (consumer_key, consumer_secret)\n",
    "\n",
    "token = eumdac.AccessToken(credentials)\n",
    "\n",
    "datastore = eumdac.DataStore(token)\n",
    "datatailor = eumdac.DataTailor(token)\n",
    "\n",
    "try:\n",
    "    print(f\"This token '{token}' expires {token.expiration}\")\n",
    "except requests.exceptions.HTTPError as error:\n",
    "    print(f\"Unexpected error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86e5fd-136e-4e1d-a95c-0796874548e5",
   "metadata": {},
   "source": [
    "# Define collection and timespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4784819f-12e4-4de8-8a15-8648930ec5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collection\n",
    "collection = 'EO:EUM:DAT:MSG:HRSEVIRI'\n",
    "\n",
    "# Set sensing start and end time\n",
    "start = datetime(2022, 1, 1, 0, 0)\n",
    "end = datetime(2022, 1, 1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b2660-20e7-483a-949a-ac96ecfe9247",
   "metadata": {},
   "source": [
    "# Get dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4eb22aa-7224-475a-ba99-c3644ca5eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EO:EUM:DAT:MSG:HRSEVIRI - High Rate SEVIRI Level 1.5 Image Data - MSG - 0 degree\n"
     ]
    }
   ],
   "source": [
    "# Select collection from datastore\n",
    "# datastore = eumdac.DataStore(token)\n",
    "\n",
    "try:    \n",
    "    selected_collection = datastore.get_collection(collection)\n",
    "    print(f\"{selected_collection} - {selected_collection.title}\")\n",
    "except eumdac.datastore.DataStoreError as error:\n",
    "    print(f\"Error related to the data store: '{error.msg}'\")\n",
    "except eumdac.collection.CollectionError as error:\n",
    "    print(f\"Error related to the collection: '{error.msg}'\")\n",
    "except requests.exceptions.ConnectionError as error:\n",
    "    print(f\"Error related to the connection: '{error.msg}'\")\n",
    "except requests.exceptions.RequestException as error:\n",
    "    print(f\"Unexpected error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec40338b-8411-42d3-a142-62ab3d5c60f0",
   "metadata": {},
   "source": [
    "# Retrieve file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa059ec8-e69c-4012-bc81-37d4bc903f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Datasets: 4 datasets for the given time range\n"
     ]
    }
   ],
   "source": [
    "# Retrieve datasets that match our filter\n",
    "products = selected_collection.search(\n",
    "    dtstart=start,\n",
    "    dtend=end)\n",
    "print(f'Found Datasets: {products.total_results} datasets for the given time range')\n",
    "\n",
    "# for product in products:\n",
    "#     try:\n",
    "#         print(product)\n",
    "#     except eumdac.collection.CollectionError as error:\n",
    "#         print(f\"Error related to the collection: '{error.msg}'\")\n",
    "#     except requests.exceptions.ConnectionError as error:\n",
    "#         print(f\"Error related to the connection: '{error.msg}'\")\n",
    "#     except requests.exceptions.RequestException as error:\n",
    "#         print(f\"Unexpected error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae65df53-f4c3-438a-b0ad-b05fb0846e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 1,\n",
       " 'data': {'kwacecile': {'disk_quota_active': True,\n",
       "   'user_quota': 20000.0,\n",
       "   'space_usage_percentage': 0.0,\n",
       "   'space_usage': 0.003981,\n",
       "   'workspace_dir_size': 0.0,\n",
       "   'log_dir_size': 0.0,\n",
       "   'output_dir_size': 0.0,\n",
       "   'nr_customisations': 0,\n",
       "   'unit_of_size': 'MB'}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " datatailor = eumdac.DataTailor(token)\n",
    "\n",
    "# To check if Data Tailor works as expected, we are requesting our quota information\n",
    "try:\n",
    "    display(datatailor.quota)\n",
    "except eumdac.datatailor.DataTailorError as error:\n",
    "    print(f\"Error related to the Data Tailor: '{error.msg}'\")\n",
    "except requests.exceptions.RequestException as error:\n",
    "    print(f\"Unexpected error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a159d-b02b-416a-ac45-0bfe4dc282bb",
   "metadata": {},
   "source": [
    "# Define bounding box area\n",
    "Here you can define which area of the entire coverage you want to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb281e5-181e-4830-8dd3-bf23de85b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box (in degrees)\n",
    "min_lon = -3.7\n",
    "max_lon = 1.35\n",
    "min_lat = 4.5\n",
    "max_lat = 11.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc14d30-8d36-4064-a0c4-d56a52332973",
   "metadata": {},
   "source": [
    "# Define chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90bfac7d-c6d9-48a8-b15e-df6903c6914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = eumdac.tailor_models.Chain(\n",
    "    product='HRSEVIRI',\n",
    "    format='msgnative',\n",
    "    filter={\"bands\": [\"channel_1\", \"channel_2\", \"channel_3\", \"channel_4\", \"channel_5\", \"channel_6\", \"channel_7\", \"channel_8\", \"channel_9\", \"channel_10\", \"channel_11\"]},\n",
    "    #projection=\"mercator\",\n",
    "    roi={\"NSWE\": [max_lat, min_lat, min_lon, max_lon]},\n",
    "    #resample_method = 'near',\n",
    "    #resample_resolution = 0.1\n",
    ")\n",
    "\n",
    "# try:\n",
    "#     datatailor.chains.create(chain)\n",
    "# except eumdac.datatailor.DataTailorError as error:\n",
    "#     print(f\"Data Tailor Error\", error)\n",
    "# except requests.exceptions.RequestException as error:\n",
    "#     print(f\"Unexpected error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "777cdb0f-2609-4d68-a8a3-7193f99e03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_SEVIRI(file_path):\n",
    "    file_path = file_path\n",
    "\n",
    "    scn = Scene(reader=\"seviri_l1b_native\", filenames=[file_path])\n",
    "    #sc = Scene(filenames=[file_path], reader=\"fci_l1c_nc\")\n",
    "    #sc = Scene(filenames=[file_path], reader=\"satpy_cf_nc\")\n",
    "    \n",
    "    #print(scn.all_dataset_names())\n",
    "\n",
    "    scn.load(['IR_016', 'IR_039', 'IR_087', 'IR_097', 'IR_108', 'IR_120', 'IR_134', 'VIS006', 'VIS008', 'WV_062', 'WV_073'])\n",
    "    return scn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f56217-97dc-454a-88c8-fa685a2043d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid_reproject(scene, min_lon, max_lon, min_lat, max_lat):\n",
    "    scn = scene\n",
    "    # Define the geographic (lat/lon) projection\n",
    "    proj_dict = {'proj': 'longlat', 'datum': 'WGS84'}\n",
    "    \n",
    "    # Calculate the resolution from the original scene's area extent and shape\n",
    "    orig_area = scn.finest_area()  # Get the finest area from the scene\n",
    "    orig_extent = orig_area.area_extent  # Original area extent (min_x, min_y, max_x, max_y)\n",
    "    orig_shape = (orig_area.width, orig_area.height)  # Original width and height in pixels\n",
    "    \n",
    "    # Calculate the resolution in degrees/pixel\n",
    "    lons, lats = scn['IR_108'].attrs['area'].get_lonlats()\n",
    "    \n",
    "    \n",
    "    lon_res = (np.min(lons) - np.max(lons)) / orig_shape[0]\n",
    "    lat_res = (np.min(lats) - np.max(lats)) / orig_shape[1]\n",
    "    \n",
    "    # Create the new area definition with the calculated resolution\n",
    "    new_area = create_area_def(\n",
    "        'my_area', proj_dict,\n",
    "        area_extent=(min_lon, min_lat, max_lon, max_lat),\n",
    "        units='degrees',\n",
    "        resolution=(lon_res, lat_res),\n",
    "    )\n",
    "\n",
    "    # Reproject the scene, maintaining original resolution\n",
    "    new_scn = scn.resample(new_area, mode='nearest', retain_values=True)\n",
    "    return new_scn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d8dcd7f-6c2f-44c2-97b2-fe672b1bf5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection(collection, start_time, end_time, credentials):\n",
    "    \n",
    "    token = eumdac.AccessToken(credentials)\n",
    "\n",
    "    datatailor = eumdac.DataTailor(token)\n",
    "    datastore = eumdac.DataStore(token)\n",
    "\n",
    "    try:    \n",
    "        selected_collection = datastore.get_collection(collection)\n",
    "        print(f\"{selected_collection} - {selected_collection.title}\")\n",
    "    except eumdac.datastore.DataStoreError as error:\n",
    "        print(f\"Error related to the data store: '{error.msg}'\")\n",
    "    except eumdac.collection.CollectionError as error:\n",
    "        print(f\"Error related to the collection: '{error.msg}'\")\n",
    "    except requests.exceptions.ConnectionError as error:\n",
    "        print(f\"Error related to the connection: '{error.msg}'\")\n",
    "    except requests.exceptions.RequestException as error:\n",
    "        print(f\"Unexpected error: {error}\")\n",
    "\n",
    "    # Retrieve datasets that match our filter\n",
    "    products = selected_collection.search(\n",
    "        dtstart=start,\n",
    "        dtend=end)\n",
    "    print(f'Found Datasets: {products.total_results} datasets for the given time range')\n",
    "    return products \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96407d94-65ab-4124-922d-9c191645ff30",
   "metadata": {},
   "source": [
    "# Linear processing\n",
    "\n",
    "The native files are downloaded and regridded and reprojected using the Satpy library. Note that currently the Native files aren't deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07c5bec8-bb13-4979-9dec-2a1ea0c68445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_api_products(products, output_dir):\n",
    "    #    Create a list of SEVIRI bands'\n",
    "    \n",
    "    sleep_time = 10\n",
    "\n",
    "    for product in products:\n",
    "        year = str(product).split('-')[5][0:4]\n",
    "        month = str(product).split('-')[5][4:6]\n",
    "        output_direc = os.path.join(output_dir, year, month)\n",
    "        os.makedirs(output_direc, exist_ok = True)\n",
    "        \n",
    "        if not os.path.exists(fr'{output_direc}\\\\{product}.hdf5'):  \n",
    "\n",
    "            try:\n",
    "                # start_c = time.time()\n",
    "                customisation = datatailor.new_customisation(product, chain)\n",
    "\n",
    "                \n",
    "                #print(f\"Customisation {customisation._id} started.\")\n",
    "            # except eumdac.datatailor.DataTailorError as error:\n",
    "            #     print(f\"Error related to the Data Tailor: '{error.msg}'\")\n",
    "            # except requests.exceptions.RequestException as error:\n",
    "            #     print(f\"Unexpected error: {error}\")\n",
    "        \n",
    "                while True:\n",
    "                    status = customisation.status\n",
    "                    if \"DONE\" in status:\n",
    "                        # stop_c = time.time()\n",
    "                        # print(f'customization took {(stop_c - start_c)} sec')\n",
    "                        # start_d = time.time()\n",
    "                        #print(f\"Customisation {customisation._id} is successfully completed.\")\n",
    "                        #print(f\"Downloading the msgnative output of the customisation {customisation._id}\")\n",
    "                        zip_files = fnmatch.filter(customisation.outputs, '*')[0]\n",
    "                        with customisation.stream_output(zip_files) as stream:\n",
    "                            # Check if stream.name (the file path) already exists\n",
    "                            if not os.path.exists(fr'{stream.name}'):\n",
    "                                # If the file doesn't exist, open it for writing\n",
    "                                with open(stream.name, mode='wb') as fdst:\n",
    "                                    shutil.copyfileobj(stream, fdst)\n",
    "        \n",
    "                                print(f\"File '{stream.name}' created and saved.\")\n",
    "                            else:\n",
    "                                print(f\"File '{stream.name}' already exists. Skipping creation.\")\n",
    "                        \n",
    "                        # stop_d = time.time()\n",
    "                        # print(f'downloading took {(stop_d - start_d)} sec')\n",
    "                        print(f\"Download finished for customisation {customisation._id}.\")\n",
    "        \n",
    "                        # start_rpj = time.time()\n",
    "                    \n",
    "        \n",
    "                        # The server only has 20 GB available for customizations, make sure to delete them after they have been dowloaded\n",
    "        \n",
    "                        break\n",
    "                    elif status in [\"ERROR\", \"FAILED\", \"DELETED\", \"KILLED\", \"INACTIVE\"]:\n",
    "                        print(f\"Customisation {customisation._id} was unsuccessful. Comisausttion log is printed.\\n\")\n",
    "                        print(customisation.logfile)\n",
    "                        try:\n",
    "                            customisation.delete()\n",
    "                        except eumdac.datatailor.CustomisationError as error:\n",
    "                            print(\"Customisation Error:\", error)\n",
    "                        except requests.exceptions.RequestException as error:\n",
    "                            print(\"Unexpected error:\", error)\n",
    "                        break\n",
    "                    elif \"QUEUED\" in status:\n",
    "                        print(f\"Customisation {customisation._id} is queued.\")\n",
    "                    # elif \"RUNNING\" in status:\n",
    "                    #     print(f\"Customisation {customisation._id} is running.\")\n",
    "                    time.sleep(sleep_time)\n",
    "        \n",
    "                file_path = fr'C:\\Users\\c.kwa\\Desktop\\meteosat_retrieval\\SEVIRI_retrieval\\{stream.name}'\n",
    "                scn = import_SEVIRI(file_path)\n",
    "                print('file imported')\n",
    "\n",
    "                customisation.delete()\n",
    "                \n",
    "                #reproject the file in right format\n",
    "                rpj_scn = regrid_reproject(scn, min_lon, max_lon, min_lat, max_lat)\n",
    "                print('file reprojected')\n",
    "        \n",
    "        \n",
    "                # Define the output path for the HDF5 file\n",
    "                #fr'C:\\Users\\c.kwa\\Desktop\\meteosat_retrieval\\SEVIRI_retrieval\\Test_batch\\Native_to_h5\\{fdst.name[:-4]}.nc'\n",
    "                output_path = fr'{output_direc}\\\\{product}.nc'\n",
    "                \n",
    "               \n",
    "                # Save to HDF5 using the NetCDF4 engine\n",
    "                rpj_scn.save_datasets(filename=output_path, engine='netcdf4')\n",
    "        \n",
    "        \n",
    "                # stop_rpj = time.time()\n",
    "                # print(f'reprojection took {stop_rpj - start_rpj} sec')\n",
    "        \n",
    "        \n",
    "                # Save the modified dataset to a new HDF5 file\n",
    "        \n",
    "                # Load with xarray, drop variables, and save to HDF5\n",
    "                with xr.open_dataset(output_path, engine='netcdf4') as ds:\n",
    "                    ds = ds.drop_vars(['longitude', 'latitude'])\n",
    "                    ds.to_netcdf(fr'{output_direc}\\\\{product}.hdf5')\n",
    "                    \n",
    "                # ds = xr.open_dataset(output_path, engine='netcdf4')\n",
    "                # ds = ds.drop_vars(['longitude', 'latitude'])\n",
    "                # ds.to_netcdf(fr'{output_dir}\\\\{product}.hdf5')\n",
    "        \n",
    "                if os.path.exists(output_path):\n",
    "                    os.remove(output_path)\n",
    "        \n",
    "                # #Removing the native file after the reprojected netcdf file is saved\n",
    "                # if os.path.exist(file_path):\n",
    "                #     os.remove(file_path)\n",
    "                    \n",
    "\n",
    "            except:\n",
    "                print('unexpected error occured')\n",
    "                for customisation in datatailor.customisations: \n",
    "                    if customisation.status in ['INACTIVE']:\n",
    "                        customisation.kill()\n",
    "                        try:\n",
    "                            customisation.delete()\n",
    "                        except eumdac.datatailor.CustomisationError as error:\n",
    "                            print(\"Customisation Error:\", error)\n",
    "                        except Exception as error:\n",
    "                            print(\"Unexpected error:\", error)\n",
    "                        \n",
    "                        print(f'Delete {customisation.status} customisation {customisation} from {customisation.creation_time} UTC.')\n",
    "            \n",
    "                    elif customisation.status in [\"ERROR\", \"FAILED\", \"DELETED\", \"KILLED\",]:\n",
    "                        try:\n",
    "                            customisation.delete()\n",
    "                        except eumdac.datatailor.CustomisationError as error:\n",
    "                            print(\"Customisation Error:\", error)\n",
    "                        except requests.exceptions.RequestException as error:\n",
    "                            print(\"Unexpected error:\", error)\n",
    "    \n",
    "                        print(f'Delete completed customisation {customisation} from {customisation.creation_time} UTC.')\n",
    "       \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31516b-1f95-4530-846b-4a45befa9fcc",
   "metadata": {},
   "source": [
    "# Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08f1b643-2702-4ca5-977a-e5b401ed7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_download_api_products(list_of_products, list_of_dirs, threads=3):\n",
    "    # Set number of threads (cores) used for parallel run and map threads\n",
    "    if threads is None:\n",
    "        pool = Pool()\n",
    "    else:\n",
    "        pool = Pool(nodes=threads)\n",
    "    # Run parallel function\n",
    "    results = pool.map( download_api_products,\n",
    "                        list_of_products, list_of_dirs)\n",
    "    \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886274ab-f98d-4bef-9d8b-b21bb7cfb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nested list of products for parallel pool\n",
    "nested_products = [[x] for x in products]\n",
    "list_of_dirs = [OUTPUTDIR] * len(nested_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07fcd896-2e7e-47ee-be02-cabe3d7628b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'MSG4-SEVI-MSG15-0100-NA-20220101005742.389000000Z-NA.subset.nat' created and saved.\n",
      "Download finished for customisation 68feb25c.\n",
      "unexpected error occured\n",
      "File 'MSG4-SEVI-MSG15-0100-NA-20220101004242.512000000Z-NA.subset.nat' already exists. Skipping creation.\n",
      "Download finished for customisation d45676e9.\n",
      "unexpected error occured\n",
      "File 'MSG4-SEVI-MSG15-0100-NA-20220101002742.635000000Z-NA.subset.nat' already exists. Skipping creation.\n",
      "Download finished for customisation 43699b61.\n",
      "unexpected error occured\n",
      "File 'MSG4-SEVI-MSG15-0100-NA-20220101001242.759000000Z-NA.subset.nat' already exists. Skipping creation.\n",
      "Download finished for customisation 5f0e09fd.\n",
      "unexpected error occured\n",
      "Execution time (minutes): 1.2685291488965353\n"
     ]
    }
   ],
   "source": [
    "# Parallel processing with timing\n",
    "start = time.time()\n",
    "parallel_download_api_products(nested_products, list_of_dirs)\n",
    "stop = time.time()\n",
    "print(f'Execution time (minutes): {(stop-start)/60}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14d0910-9cf0-4d06-be38-203fbf68556b",
   "metadata": {},
   "source": [
    "# Cleaning your workspace\n",
    "Sometimes, because of multiple failed download files, your workspace exceeds the maximum number of 25 GB. With this code you can clean your online workspace, to make room for new download requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "913241b9-0b62-4bc9-8609-2cca3ce62c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete completed customisation 5f0e09fd from 2025-03-05 09:32:03 UTC.\n",
      "Delete completed customisation d45676e9 from 2025-03-05 09:31:25 UTC.\n",
      "Delete completed customisation 68feb25c from 2025-03-05 09:31:25 UTC.\n",
      "Delete completed customisation 43699b61 from 2025-03-05 09:31:25 UTC.\n"
     ]
    }
   ],
   "source": [
    " # Clearing all customisations from the Data Tailor\n",
    "\n",
    "for customisation in datatailor.customisations:\n",
    "    if customisation.status in ['QUEUED', 'INACTIVE', 'RUNNING']:\n",
    "        customisation.kill()\n",
    "        print(f'Delete {customisation.status} customisation {customisation} from {customisation.creation_time} UTC.')\n",
    "        try:\n",
    "            customisation.delete()\n",
    "        except eumdac.datatailor.CustomisationError as error:\n",
    "            print(\"Customisation Error:\", error)\n",
    "        except Exception as error:\n",
    "            print(\"Unexpected error:\", error)\n",
    "    else:\n",
    "        print(f'Delete completed customisation {customisation} from {customisation.creation_time} UTC.')\n",
    "        try:\n",
    "            customisation.delete()\n",
    "        except eumdac.datatailor.CustomisationError as error:\n",
    "            print(\"Customisation Error:\", error)\n",
    "        except requests.exceptions.RequestException as error:\n",
    "            print(\"Unexpected error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f94d2-eb9d-4557-a0d0-143fdba883e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
